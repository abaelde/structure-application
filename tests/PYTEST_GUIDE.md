# Guide d'utilisation de Pytest

## ğŸ‰ Installation

Pytest est maintenant installÃ© et configurÃ© dans le projet !

```bash
âœ… pytest==8.4.2
âœ… pytest-cov==7.0.0
```

---

## ğŸš€ Commandes de base

### ExÃ©cuter tous les tests

```bash
uv run pytest
```

### ExÃ©cuter avec verbositÃ©

```bash
uv run pytest -v
```

### ExÃ©cuter uniquement les tests unitaires

```bash
uv run pytest tests/unit/
```

### ExÃ©cuter uniquement les tests d'intÃ©gration

```bash
uv run pytest tests/integration/
```

### ExÃ©cuter un fichier de test spÃ©cifique

```bash
uv run pytest tests/unit/loaders/test_bordereau_validation.py
```

### ExÃ©cuter un test spÃ©cifique

```bash
uv run pytest tests/unit/loaders/test_bordereau_validation.py::test_valid_bordereau
```

---

## ğŸ“Š Couverture de code

### GÃ©nÃ©rer un rapport de couverture dans le terminal

```bash
uv run pytest --cov=src
```

### GÃ©nÃ©rer un rapport HTML dÃ©taillÃ©

```bash
uv run pytest --cov=src --cov-report=html
```

Le rapport HTML sera gÃ©nÃ©rÃ© dans `htmlcov/index.html`. Ouvrez-le dans un navigateur :

```bash
open htmlcov/index.html
```

### Afficher les lignes manquantes

```bash
uv run pytest --cov=src --cov-report=term-missing
```

### Couverture minimale requise

```bash
uv run pytest --cov=src --cov-fail-under=65
```

Cela fera Ã©chouer les tests si la couverture est infÃ©rieure Ã  65%.

---

## ğŸ” Options utiles

### Afficher les prints pendant les tests

Par dÃ©faut, pytest capture les prints. Pour les afficher :

```bash
uv run pytest -s
```

### ArrÃªter au premier Ã©chec

```bash
uv run pytest -x
```

### ArrÃªter aprÃ¨s N Ã©checs

```bash
uv run pytest --maxfail=3
```

### ExÃ©cuter les tests qui ont Ã©chouÃ© lors de la derniÃ¨re exÃ©cution

```bash
uv run pytest --lf
```

### ExÃ©cuter d'abord les tests qui ont Ã©chouÃ©

```bash
uv run pytest --ff
```

### Mode "watch" avec pytest-watch (Ã  installer)

```bash
# Installation
uv add --dev pytest-watch

# Utilisation
uv run ptw
```

---

## ğŸ·ï¸ Markers (tags de tests)

### DÃ©finir des markers

Dans `pyproject.toml`, les markers suivants sont dÃ©finis :

```toml
markers = [
    "unit: Tests unitaires (fonctions isolÃ©es)",
    "integration: Tests d'intÃ©gration (workflow complets)",
    "slow: Tests lents",
]
```

### Utiliser un marker

Dans votre test :

```python
import pytest

@pytest.mark.unit
def test_quota_share():
    # ...

@pytest.mark.integration
def test_full_workflow():
    # ...

@pytest.mark.slow
def test_heavy_computation():
    # ...
```

### ExÃ©cuter seulement les tests avec un marker

```bash
# Seulement les tests unitaires
uv run pytest -m unit

# Seulement les tests d'intÃ©gration
uv run pytest -m integration

# Exclure les tests lents
uv run pytest -m "not slow"
```

---

## ğŸ“ Configuration (pyproject.toml)

La configuration de pytest se trouve dans `pyproject.toml` :

```toml
[tool.pytest.ini_options]
testpaths = ["tests"]              # OÃ¹ chercher les tests
python_files = "test_*.py"         # Pattern des fichiers de test
python_classes = "Test*"           # Pattern des classes de test
python_functions = "test_*"        # Pattern des fonctions de test
addopts = [
    "-v",                          # Verbeux par dÃ©faut
    "--tb=short",                  # Traceback court
    "--strict-markers",            # Erreur si marker inconnu
    "--cov=src",                   # Couverture du dossier src
    "--cov-report=term-missing",   # Afficher lignes manquantes
    "--cov-report=html",           # GÃ©nÃ©rer rapport HTML
]
```

---

## ğŸ§ª Ã‰tat actuel des tests

### RÃ©sumÃ©

```
âœ… 11 tests passent
ğŸ“Š 65% de couverture de code
```

### Tests par catÃ©gorie

| CatÃ©gorie | Nombre | Statut |
|-----------|--------|--------|
| **Tests unitaires** | 9 | âœ… Tous passent |
| **Tests d'intÃ©gration** | 2 | âœ… Tous passent |

### DÃ©tail par fichier

#### Tests unitaires (`tests/unit/`)

**`loaders/test_bordereau_validation.py`** - 9 tests
- âœ… `test_valid_bordereau`
- âœ… `test_bordereau_with_warnings`
- âœ… `test_invalid_bordereau`
- âœ… `test_missing_required_columns`
- âœ… `test_unknown_columns`
- âœ… `test_only_required_columns`
- âœ… `test_partial_dimensions`
- âœ… `test_null_dimension_columns`
- âœ… `test_null_required_columns`

**Couverture:**
- `bordereau_validator.py`: 93% âœ…
- `bordereau_loader.py`: 86% âœ…

#### Tests d'intÃ©gration (`tests/integration/`)

**`test_policy_lifecycle.py`** - 1 test
- âœ… `test_policy_expiry_mechanism`

**Couverture:**
- `policy_lifecycle.py`: 89% âœ…
- `calculation_engine.py`: 95% âœ…

**`test_exclusion_mechanism.py`** - 1 test
- âœ… `test_exclusion_mechanism`

**Couverture:**
- `condition_matcher.py`: 51% âš ï¸
- `structure_orchestrator.py`: 63% âš ï¸

---

## ğŸ“ˆ Couverture par module

Modules les mieux couverts :

| Module | Couverture | Statut |
|--------|------------|--------|
| `calculation_engine.py` | 95% | âœ… |
| `bordereau_validator.py` | 93% | âœ… |
| `policy_lifecycle.py` | 89% | âœ… |
| `program_loader.py` | 89% | âœ… |
| `bordereau_loader.py` | 86% | âœ… |
| `exposure_mapping.py` | 100% | âœ… |

Modules nÃ©cessitant plus de tests :

| Module | Couverture | Tests manquants |
|--------|------------|-----------------|
| `excess_of_loss.py` | 14% | âŒ Tests unitaires |
| `quota_share.py` | 67% | âŒ Tests unitaires |
| `cession_calculator.py` | 64% | âŒ Tests unitaires |
| `condition_matcher.py` | 51% | âš ï¸ Tests unitaires |
| `bordereau_processor.py` | 37% | âš ï¸ Tests d'intÃ©gration |
| `treaty_manager.py` | 23% | âŒ Tests unitaires |
| `report_display.py` | 9% | âŒ Tests (basse prioritÃ©) |

---

## ğŸ› ï¸ Fixtures communes (conftest.py)

Des fixtures sont disponibles dans `tests/conftest.py` :

```python
import pytest

def test_example(sample_valid_bordereau_data):
    # sample_valid_bordereau_data : DataFrame de bordereau valide
    # ...
```

### Fixtures disponibles

- `sample_bordereau_path`: Chemin vers `bordereau_exemple.csv`
- `sample_valid_bordereau_data`: DataFrame avec donnÃ©es de bordereau valides

### Builders disponibles

Pour crÃ©er des programmes de test, utiliser les **builders** au lieu de fichiers Excel :

```python
from tests.builders import build_quota_share, build_program

qs = build_quota_share(name="QS_30", cession_pct=0.30)
program = build_program(name="TEST", structures=[qs])
```

Voir `tests/builders/README.md` pour la documentation complÃ¨te.

---

## ğŸ“‹ Exemple de crÃ©ation de test

### Test unitaire simple

```python
# tests/unit/domain/products/test_quota_share.py

import pytest
from src.domain.products import quota_share


def test_quota_share_without_limit():
    """Test quota share sans limite"""
    exposure = 1000.0
    cession_pct = 0.3
    
    result = quota_share(exposure, cession_pct)
    
    assert result == 300.0


def test_quota_share_with_limit_not_reached():
    """Test quota share avec limite non atteinte"""
    exposure = 1000.0
    cession_pct = 0.3
    limit = 500.0
    
    result = quota_share(exposure, cession_pct, limit)
    
    assert result == 300.0  # 30% de 1000 = 300 < 500


def test_quota_share_with_limit_reached():
    """Test quota share avec limite atteinte"""
    exposure = 2000.0
    cession_pct = 0.5
    limit = 500.0
    
    result = quota_share(exposure, cession_pct, limit)
    
    assert result == 500.0  # 50% de 2000 = 1000 > 500, donc capped


def test_quota_share_invalid_cession_rate():
    """Test avec taux de cession invalide"""
    with pytest.raises(ValueError, match="Cession rate must be between 0 and 1"):
        quota_share(1000.0, 1.5)


def test_quota_share_negative_limit():
    """Test avec limite nÃ©gative"""
    with pytest.raises(ValueError, match="Limit must be positive"):
        quota_share(1000.0, 0.3, limit=-100)
```

### Test d'intÃ©gration avec builders

```python
# tests/integration/test_full_workflow.py

import pandas as pd
from src.engine import apply_program_to_bordereau
from tests.builders import build_quota_share, build_program


def test_full_workflow():
    """Test du workflow complet de A Ã  Z"""
    # Arrange - CrÃ©er le programme avec les builders
    qs = build_quota_share(name="QS_30", cession_pct=0.30)
    program = build_program(name="TEST", structures=[qs])
    
    # CrÃ©er un bordereau de test
    bordereau_df = pd.DataFrame({
        "policy_id": ["POL-001", "POL-002"],
        "INSURED_NAME": ["COMPANY A", "COMPANY B"],
        "exposure": [1_000_000, 2_000_000],
        "INCEPTION_DT": ["2024-01-01", "2024-01-01"],
        "EXPIRE_DT": ["2025-01-01", "2025-01-01"],
        "BUSCL_COUNTRY_CD": ["US", "FR"],
        "BUSCL_LIMIT_CURRENCY_CD": ["USD", "EUR"],
        # ... autres colonnes requises
    })
    
    # Act
    calculation_date = "2024-06-01"
    bordereau_with_cession, results = apply_program_to_bordereau(
        bordereau_df, program, calculation_date=calculation_date
    )
    
    # Assert
    assert len(results) == 2
    assert "cession_to_reinsurer" in results.columns
    assert results["cession_to_reinsurer"].sum() > 0
```

---

## ğŸ¯ Bonnes pratiques

### 1. Nommage des tests

- âœ… `test_quota_share_with_limit_reached`
- âœ… `test_invalid_bordereau_missing_columns`
- âŒ `test1`, `test_func`

### 2. Structure Arrange-Act-Assert

```python
def test_example():
    # Arrange: PrÃ©parer les donnÃ©es
    input_data = ...
    expected = ...
    
    # Act: ExÃ©cuter la fonction
    result = function(input_data)
    
    # Assert: VÃ©rifier le rÃ©sultat
    assert result == expected
```

### 3. Un test = une chose testÃ©e

- âœ… Un test par scÃ©nario
- âŒ Tester plusieurs scÃ©narios dans le mÃªme test

### 4. Tests indÃ©pendants

- Chaque test doit pouvoir s'exÃ©cuter seul
- Pas de dÃ©pendance entre tests
- Utiliser des fixtures pour partager du code

### 5. Messages d'assertion clairs

```python
# âœ… Bon
assert result == 300.0, f"Expected 300.0 but got {result}"

# âœ… Encore mieux avec pytest
assert result == 300.0  # pytest montre automatiquement les valeurs
```

---

## ğŸš¦ CI/CD (Ã  venir)

Pour l'intÃ©gration continue, ajouter dans `.github/workflows/tests.yml` :

```yaml
name: Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      - name: Install uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh
      - name: Install dependencies
        run: uv sync
      - name: Run tests
        run: uv run pytest --cov=src --cov-fail-under=65
```

---

## ğŸ“š Ressources

- [Documentation officielle de pytest](https://docs.pytest.org/)
- [pytest-cov documentation](https://pytest-cov.readthedocs.io/)
- [Guide des fixtures pytest](https://docs.pytest.org/en/stable/fixture.html)
- [Guide des markers pytest](https://docs.pytest.org/en/stable/example/markers.html)

---

**Document crÃ©Ã© le 13 octobre 2025**

